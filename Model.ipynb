{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-Y3uL4dnLlZ"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install torchmetrics\n",
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_VSxW23QItI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq1f6T6FB5uQ"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"dataset_dir\": \"dataset\",\n",
        "    \"glove_dir\": \"glove_embeddings\",\n",
        "    \"image_specs\": {\n",
        "        \"img_feature_channels\": 2048,\n",
        "        \"image_size\": 224\n",
        "    },\n",
        "    \"embeddings\": {\n",
        "        \"size\": 17\n",
        "    },\n",
        "    \"vocab_size\": 17,\n",
        "\n",
        "    \"PAD_token\": \"<pad>\",\n",
        "    \"PAD_idx\": 0,\n",
        "\n",
        "    \"START_idx\": 1,\n",
        "    \"START_token\": \"<start>\",\n",
        "\n",
        "    \"END_idx\": 2,\n",
        "    \"END_token\": \"<end>\",\n",
        "\n",
        "    \"UNK_idx\": 3,\n",
        "    \"UNK_token\": \"<unk>\",\n",
        "\n",
        "    \"max_len\": 256,\n",
        "\n",
        "    \"use_gpu\": True,\n",
        "    \"seed\": 2021,\n",
        "\n",
        "    \"batch_size\": {\n",
        "        \"train\": 128,\n",
        "        \"eval\": 64\n",
        "    },\n",
        "\n",
        "    \"model_configuration\": {\n",
        "        \"decoder_layers\": 6,\n",
        "        \"d_model\": 16,\n",
        "        \"ff_dim\": 1024,\n",
        "        \"attention_heads\": 16,\n",
        "        \"dropout\": 0.5\n",
        "    },\n",
        "\n",
        "    \"train_config\": {\n",
        "        \"num_of_epochs\": 30,\n",
        "        \"warmup_steps\": 2811,\n",
        "        \"l2_penalty\": 0.5,\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"gradient_clipping\": 2.0,\n",
        "        \"eval_period\": 3\n",
        "    },\n",
        "\n",
        "    \"bleu_weights\": {\n",
        "        \"bleu-1\": [1.0],\n",
        "        \"bleu-2\": [0.5, 0.5],\n",
        "        \"bleu-3\": [0.333, 0.333, 0.333],\n",
        "        \"bleu-4\": [0.25, 0.25, 0.25, 0.25]\n",
        "    },\n",
        "\n",
        "    \"checkpoint\": {\n",
        "        \"load\": False,\n",
        "        \"checkpoint_path\": \".\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRq4NQnVQItN"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyrBlq2LQItN"
      },
      "outputs": [],
      "source": [
        "class PositionalEncodings(nn.Module):\n",
        "    \"\"\"Attention is All You Need positional encoding layer\"\"\"\n",
        "\n",
        "    def __init__(self, seq_len, d_model, p_dropout):\n",
        "        \"\"\"Initializes the layer.\"\"\"\n",
        "        super(PositionalEncodings, self).__init__()\n",
        "        token_positions = torch.arange(start=0, end=seq_len).view(-1, 1)\n",
        "        dim_positions = torch.arange(start=0, end=d_model).view(1, -1)\n",
        "        angles = token_positions / (10000 ** ((2 * dim_positions) / d_model))\n",
        "\n",
        "        encodings = torch.zeros(1, seq_len, d_model)\n",
        "        encodings[0, :, ::2] = torch.cos(angles[:, ::2])\n",
        "        encodings[0, :, 1::2] = torch.sin(angles[:, 1::2])\n",
        "        encodings.requires_grad = False\n",
        "        self.register_buffer(\"positional_encodings\", encodings)\n",
        "\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Performs forward pass of the module.\"\"\"\n",
        "        x = x + self.positional_encodings\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEFZFqQ4QItO"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder for image captions.\n",
        "\n",
        "    Generates prediction for next caption word given the prviously\n",
        "    generated word and image features extracted from CNN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Initializes the model.\"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        model_config = config[\"model_configuration\"]\n",
        "        decoder_layers = model_config[\"decoder_layers\"]\n",
        "        attention_heads = model_config[\"attention_heads\"]\n",
        "        d_model = model_config[\"d_model\"]\n",
        "        ff_dim = model_config[\"ff_dim\"]\n",
        "        dropout = model_config[\"dropout\"]\n",
        "\n",
        "        embedding_dim = config[\"embeddings\"][\"size\"]\n",
        "        vocab_size = config[\"vocab_size\"]\n",
        "        img_feature_channels = config[\"image_specs\"][\"img_feature_channels\"]\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size+1, d_model)\n",
        "\n",
        "        self.entry_mapping_tokens = nn.Linear(d_model, d_model)\n",
        "        self.entry_mapping_img = nn.Linear(img_feature_channels, d_model)\n",
        "\n",
        "        self.res_block = ResidualBlock(d_model)\n",
        "\n",
        "        self.positional_encodings = PositionalEncodings(config[\"max_len\"], d_model, dropout)\n",
        "        transformer_decoder_layer = TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=attention_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.decoder = TransformerDecoder(transformer_decoder_layer, decoder_layers)\n",
        "        self.classifier = nn.Linear(d_model, vocab_size + 1)\n",
        "\n",
        "    def forward(self, x, image_features, tgt_padding_mask=None, tgt_mask=None):\n",
        "        \"\"\"Performs forward pass of the module.\"\"\"\n",
        "        # Adapt the dimensionality of the features for image patches\n",
        "        image_features = self.entry_mapping_img(image_features)\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "        image_features = F.leaky_relu(image_features)\n",
        "\n",
        "        # Inside the forward method of your Decoder class\n",
        "        # print(\"Before embedding layer - Shape:\", x.shape, \"Minimum index:\", torch.min(x), \"Maximum index:\", torch.max(x))\n",
        "        x = self.embedding_layer(x)\n",
        "        # print(\"After embedding layer - Shape:\", x.shape, \"Minimum index:\", torch.min(x), \"Maximum index:\", torch.max(x))\n",
        "\n",
        "        # TODO: uncomment this???\n",
        "        # Ensure that indices are within the valid range\n",
        "        # assert torch.min(x) >= 0, \"Minimum index should be non-negative\"\n",
        "        # assert torch.max(x) < self.embedding_layer.num_embeddings, \"Maximum index exceeds vocabulary size\"\n",
        "\n",
        "        # x = self.entry_mapping_tokens(x)\n",
        "        # x = F.leaky_relu(x)\n",
        "\n",
        "        x = self.res_block(x)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        x = self.positional_encodings(x)\n",
        "\n",
        "        # Get output from the decoder\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.decoder(\n",
        "            tgt=x,\n",
        "            memory=image_features,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4290vhWfQItP"
      },
      "outputs": [],
      "source": [
        "def set_up_causal_mask(seq_len, device):\n",
        "    \"\"\"Defines the triangular mask used in transformers.\n",
        "\n",
        "    This mask prevents decoder from attending the tokens after the current one.\n",
        "\n",
        "    Arguments:\n",
        "        seq_len (int): Maximum length of input sequence\n",
        "        device: Device on which to map the created tensor mask\n",
        "    Returns:\n",
        "        mask (torch.Tensor): Created triangular mask\n",
        "    \"\"\"\n",
        "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
        "    mask.requires_grad = False\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCvpgp6fyCO0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import re\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "class CustomImageDataset():\n",
        "    def __init__(self, img_dir, text_dir, transform=None, target_transform=None):\n",
        "        self.text_dir = text_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1700\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.img_dir, f\"{idx}.png\")\n",
        "\n",
        "        # Open the image using PIL and convert it to RGB\n",
        "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Use transforms.ToTensor() to convert the PIL image to a tensor\n",
        "        image = transforms.ToTensor()(image_pil)\n",
        "\n",
        "        text_path = os.path.join(self.text_dir, f\"{idx}.gui\")\n",
        "        tgt_padding_mask = None\n",
        "\n",
        "        with open(text_path, 'r') as file:\n",
        "            content = file.read()\n",
        "            content.replace(',', '  , ')\n",
        "            label = re.split(r'\\s+|\\n', content)\n",
        "            label = list(filter(None, label))\n",
        "\n",
        "        label = tokenize(label)\n",
        "        start_token = tokenize(['<START>'])\n",
        "        end_token = tokenize(['<END>'])\n",
        "        tokens = start_token + label + end_token\n",
        "\n",
        "        input_tokens = tokens[:-1].copy()\n",
        "        tgt_tokens = tokens[1:].copy()\n",
        "\n",
        "        sample_size = len(input_tokens)\n",
        "        padding_size = config['max_len'] - sample_size\n",
        "\n",
        "        if padding_size > 0:\n",
        "            padding_vec = [0 for _ in range(padding_size)]\n",
        "            input_tokens += padding_vec.copy()\n",
        "            tgt_tokens += padding_vec.copy()\n",
        "\n",
        "        input_tokens = torch.Tensor(input_tokens).long()\n",
        "        tgt_tokens = torch.Tensor(tgt_tokens).long()\n",
        "\n",
        "        tgt_padding_mask = torch.ones([config['max_len'], ])\n",
        "        tgt_padding_mask[:sample_size] = 0.0\n",
        "        tgt_padding_mask = tgt_padding_mask.bool()\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # print(image.shape, input_tokens.shape, tgt_tokens.shape, tgt_padding_mask.shape)\n",
        "        return image, input_tokens, tgt_tokens, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "class CustomSubset(Subset):\n",
        "    def __init__(self, dataset, indices):\n",
        "        super().__init__(dataset, indices)\n",
        "\n",
        "    def get_random_subset(self, subset_size):\n",
        "        num_samples = len(self)\n",
        "        subset_indices = random.sample(range(num_samples), subset_size)\n",
        "        subset = [self[i] for i in subset_indices]\n",
        "        return subset"
      ],
      "metadata": {
        "id": "8dBNz02C2zsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_zZXnuN2kGi"
      },
      "outputs": [],
      "source": [
        "sentence = \"<PADDING> , { } small-title text quadruple row btn-inactive btn-orange btn-green btn-red double <START> header btn-active <END> single\"\n",
        "vocabulary = sentence.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc8efIF95d4d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Pjv1Hh2bao"
      },
      "outputs": [],
      "source": [
        "def tokenize(code):\n",
        "    if isinstance(code, str):\n",
        "        # Assuming code is a JSON-like string, you can use json.loads to parse it\n",
        "        codels = json.loads(code)\n",
        "    elif isinstance(code, list):\n",
        "        codels = []\n",
        "        for word in code:\n",
        "            splitted = word.split(',')\n",
        "            codels.extend(splitted)\n",
        "        codels = list(filter(lambda x: x!= '', codels))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported type for 'code'. It should be either a string or a list.\")\n",
        "\n",
        "    token_list = [vocabulary.index(item) for item in codels if item in vocabulary]\n",
        "\n",
        "    return token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzvmriVh2cTv"
      },
      "outputs": [],
      "source": [
        "def untokenize(tokens):\n",
        "  word = []\n",
        "  for token in tokens:\n",
        "    word.append(vocabulary[token])\n",
        "\n",
        "  return (\" \").join(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THz7vvHs0wGF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decoding(model, img_features_batched, sos_id, eos_id, pad_id, idx2word, max_len, device):\n",
        "    \"\"\"Performs greedy decoding for the caption generation.\n",
        "    At each iteration model predicts the next word in the caption given the previously\n",
        "    generated words and image features. For the next word we always take the most probable one.\n",
        "    Arguments:\n",
        "        model (torch.nn.Module): Transformer Decoder model which generates prediction for the next word\n",
        "        img_features_padded (torch.Tensor): Image features generated by CNN encoder\n",
        "            Stacked along 0-th dimension for each image in the mini-batch\n",
        "        sos_id (int): Id of <start> token in the vocabulary\n",
        "        eos_id (int): Id of <end> token in the vocabulary\n",
        "        pad_id (int): Id of <pad> token in the vocabulary\n",
        "        idx2word (dict): Mapping from ordinal number of token (i.e. class number) to the string of word\n",
        "        max_len (int): Maximum length of the caption\n",
        "        device (torch.device): Device on which to port used tensors\n",
        "    Returns:\n",
        "        generated_captions (list of str): Captions generated for each image in the batch\n",
        "    \"\"\"\n",
        "    batch_size = img_features_batched.size(0)\n",
        "\n",
        "    print(batch_size)\n",
        "\n",
        "    # Define the initial state of decoder input\n",
        "    x_words = torch.Tensor([sos_id] + [pad_id] * (max_len - 1)).to(device).long()\n",
        "    x_words = x_words.repeat(batch_size, 1)\n",
        "    padd_mask = torch.Tensor([True] * max_len).to(device).bool()\n",
        "    padd_mask = padd_mask.repeat(batch_size, 1)\n",
        "\n",
        "    # Is each image from the batch decoded\n",
        "    is_decoded = [False] * batch_size\n",
        "    generated_captions = []\n",
        "    for _ in range(batch_size):\n",
        "        generated_captions.append([])\n",
        "\n",
        "    for i in range(max_len - 1):\n",
        "        # Update the padding masks\n",
        "        padd_mask[:, i] = False\n",
        "\n",
        "        # Get the model prediction for the next word\n",
        "        y_pred_prob = model(x_words, img_features_batched, padd_mask)\n",
        "        # Extract the prediction from the specific (next word) position of the target sequence\n",
        "        y_pred_prob = y_pred_prob[torch.arange(batch_size), [i] * batch_size].clone()\n",
        "        # Extract the most probable word\n",
        "        y_pred = y_pred_prob.argmax(-1)\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            print(str(y_pred[batch_idx].item()))\n",
        "            if is_decoded[batch_idx]:\n",
        "                continue\n",
        "            # Add the generated word to the caption\n",
        "            generated_captions[batch_idx].append(idx2word[y_pred[batch_idx].item()])\n",
        "            if y_pred[batch_idx] == eos_id:\n",
        "                # Caption has been fully generated for this image\n",
        "                is_decoded[batch_idx] = True\n",
        "\n",
        "        if np.all(is_decoded):\n",
        "            break\n",
        "\n",
        "        if i < (max_len - 1):   # We haven't reached maximum number of decoding steps\n",
        "            # Update the input tokens for the next iteration\n",
        "            x_words[torch.arange(batch_size), [i+1] * batch_size] = y_pred.view(-1)\n",
        "\n",
        "    # Complete the caption for images which haven't been fully decoded\n",
        "    for batch_idx in range(batch_size):\n",
        "        if not is_decoded[batch_idx]:\n",
        "            generated_captions[batch_idx].append(idx2word[eos_id])\n",
        "\n",
        "    # # Clean the EOS symbol\n",
        "    # for caption in generated_captions:\n",
        "    #     caption.remove(\"<END>\")\n",
        "\n",
        "    return generated_captions"
      ],
      "metadata": {
        "id": "jlbsmNrK6eJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate(subset, encoder, decoder, config, device):\n",
        "    \"\"\"Evaluates (BLEU score) caption generation model on a given subset.\n",
        "\n",
        "    Arguments:\n",
        "        subset (CustomDataset): Train/Val/Test subset\n",
        "        encoder (nn.Module): CNN which generates image features\n",
        "        decoder (nn.Module): Transformer Decoder which generates captions for images\n",
        "        config (object): Contains configuration for the evaluation pipeline\n",
        "        device (torch.device): Device on which to port used tensors\n",
        "    Returns:\n",
        "        bleu (float): BLEU-{1:4} scores performance metric on the entire subset - corpus bleu\n",
        "    \"\"\"\n",
        "    batch_size = config[\"batch_size\"][\"eval\"]\n",
        "    max_len = config[\"max_len\"]\n",
        "    bleu_w = config[\"bleu_weights\"]\n",
        "\n",
        "    # Mapping from vocab index to string representation\n",
        "    idx2word = dict(zip(range(18), vocabulary))\n",
        "    # Ids for special tokens\n",
        "    sos_id = 13\n",
        "    eos_id = 16\n",
        "    pad_id = 0\n",
        "\n",
        "    references_total = []\n",
        "    predictions_total = []\n",
        "\n",
        "    print(\"Evaluating model.\")\n",
        "    for x_img, _, y_caption, _ in subset.get_random_subset(batch_size):\n",
        "        x_img = x_img.to(device)\n",
        "\n",
        "        # Extract image features\n",
        "        img_features = encoder(x_img)\n",
        "        img_features = img_features.view(img_features.size(0), img_features.size(1), -1)\n",
        "        img_features = img_features.permute(0, 2, 1)\n",
        "        img_features = img_features.detach()\n",
        "\n",
        "        # Get the caption prediction for each image in the mini-batch\n",
        "        predictions = greedy_decoding(decoder, img_features, sos_id, eos_id, pad_id, idx2word, max_len, device)\n",
        "        references_total += y_caption\n",
        "        predictions_total += predictions\n",
        "\n",
        "    # Evaluate BLEU score of the generated captions\n",
        "    bleu_1 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-1\"]) * 100\n",
        "    bleu_2 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-2\"]) * 100\n",
        "    bleu_3 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-3\"]) * 100\n",
        "    bleu_4 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-4\"]) * 100\n",
        "    bleu = [bleu_1, bleu_2, bleu_3, bleu_4]\n",
        "    return bleu\n"
      ],
      "metadata": {
        "id": "uG1Snl7alLcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PV7XDGbQItP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train(decoder, config, writer, device):\n",
        "    \"\"\"Performs the training of the model.\n",
        "\n",
        "    Arguments:\n",
        "        config (object): Contains configuration of the pipeline\n",
        "        writer: tensorboardX writer object\n",
        "        device: device on which to map the model and data\n",
        "    \"\"\"\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    np.random.seed(config[\"seed\"])\n",
        "\n",
        "   # Define dataloader hyper-parameters\n",
        "    train_hyperparams = {\n",
        "        \"batch_size\": config[\"batch_size\"][\"train\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 4,\n",
        "        \"drop_last\": True\n",
        "    }\n",
        "\n",
        "    # Create dataloaders\n",
        "    full_set = CustomImageDataset(img_dir='/content/drive/MyDrive/Training/img_dir', text_dir='/content/drive/MyDrive/Training/gui_dir', transform=torchvision.transforms.Resize((224, 224)))\n",
        "\n",
        "    train_indices, test_indices= train_test_split(\n",
        "        range(len(full_set)),\n",
        "        test_size = 0.1,\n",
        "        random_state = 42\n",
        "    )\n",
        "\n",
        "    train_split = CustomSubset(full_set, train_indices)\n",
        "    test_split = CustomSubset(full_set, test_indices)\n",
        "\n",
        "    train_loader = DataLoader(train_split, batch_size=config[\"batch_size\"][\"train\"], shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_split, batch_size=config[\"batch_size\"][\"eval\"], num_workers=4)\n",
        "\n",
        "    #######################\n",
        "    # Set up the encoder\n",
        "    #######################\n",
        "    # Download pretrained CNN encoder\n",
        "    encoder = models.resnet50(pretrained=True)\n",
        "    # Extract only the convolutional backbone of the model\n",
        "    encoder = torch.nn.Sequential(*(list(encoder.children())[:-2]))\n",
        "    encoder = encoder.to(device)\n",
        "    # Freeze encoder layers\n",
        "    for param in encoder.parameters():\n",
        "      param.requires_grad = False\n",
        "    encoder.eval()\n",
        "\n",
        "    ######################\n",
        "    # Set up the decoder\n",
        "    ######################\n",
        "    # Instantiate the decoder\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    # summary(decoder, input_size=(512, 1024))\n",
        "\n",
        "    # Set up causal mask for transformer decoder\n",
        "    causal_mask = set_up_causal_mask(config[\"max_len\"], device)\n",
        "\n",
        "    # Load training configuration\n",
        "    train_config = config[\"train_config\"]\n",
        "    learning_rate = train_config[\"learning_rate\"]\n",
        "\n",
        "    # Prepare the model optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        decoder.parameters(),\n",
        "        lr=train_config[\"learning_rate\"],\n",
        "        weight_decay=train_config[\"l2_penalty\"]\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=1, threshold=0.01)\n",
        "    # Loss function\n",
        "    loss_fcn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    start_time = time.strftime(\"%b-%d_%H-%M-%S\")\n",
        "    train_step = 0\n",
        "    for epoch in range(train_config[\"num_of_epochs\"]):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        decoder.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        for x_img, x_words, y, tgt_padding_mask in train_loader:\n",
        "            # print(x_img.shape, x_words.shape, y.shape)\n",
        "            optimizer.zero_grad()\n",
        "            train_step += 1\n",
        "\n",
        "            # Move the used tensors to defined device\n",
        "            x_img, x_words = x_img.to(device), x_words.to(device)\n",
        "            y = y.to(device)\n",
        "            tgt_padding_mask = tgt_padding_mask.to(device)\n",
        "\n",
        "            x_img = x_img.unsqueeze(1)\n",
        "\n",
        "            # Extract image features\n",
        "\n",
        "            with torch.no_grad():\n",
        "                img_features = encoder(x_img)\n",
        "                img_features = img_features.view(img_features.size(0), img_features.size(1), -1)\n",
        "                img_features = img_features.permute(0, 2, 1)\n",
        "                img_features = img_features.detach()\n",
        "\n",
        "            '''\n",
        "            img_features = encoder(x_img)\n",
        "            img_features = img_features.view(img_features.size(0), img_features.size(1), -1)\n",
        "            img_features = img_features.permute(0, 2, 1)\n",
        "            img_features = img_features.detach()\n",
        "            '''\n",
        "            # Get the prediction of the decoder\n",
        "            y_pred = decoder(x_words, img_features)\n",
        "            tgt_padding_mask = torch.logical_not(tgt_padding_mask)\n",
        "            y_pred = y_pred[tgt_padding_mask]\n",
        "\n",
        "            y = y[tgt_padding_mask]\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = loss_fcn(y_pred, y.long())\n",
        "            # Update model weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            writer.add_scalar(\"Train/Step-Loss\", loss.item(), train_step)\n",
        "            writer.add_scalar(\"Train/Learning-Rate\", learning_rate, train_step)\n",
        "            print(\"Train/Step-Loss\", loss.item())\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        # Evaluate model performance\n",
        "        with torch.no_grad():\n",
        "            encoder.eval()\n",
        "            decoder.eval()\n",
        "\n",
        "            # Evaluate model performance on subsets\n",
        "            train_bleu = evaluate(train_split, encoder, decoder, config, device)\n",
        "            test_bleu = evaluate(test_split, encoder, decoder, config, device)\n",
        "\n",
        "            # Log the evaluated BLEU score\n",
        "            for i, t_b in enumerate(train_bleu):\n",
        "                writer.add_scalar(f\"Train/BLEU-{i+1}\", t_b, epoch)\n",
        "\n",
        "            decoder.train()\n",
        "\n",
        "        scheduler.step(sum(train_bleu)/len(train_bleu))\n",
        "        scheduler.step(total_loss / num_batches)\n",
        "        print(\"Learning rate\", optimizer.param_groups[0][\"lr\"])\n",
        "    torch.save(decoder.state_dict(), '/content/drive/MyDrive/Training/modelTest.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT_2oZl2bwst"
      },
      "outputs": [],
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "import json\n",
        "\n",
        "writer = SummaryWriter(log_dir='./logs')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# with open('/content/drive/MyDrive/Training/config.json', 'r') as f:\n",
        "    # config = json.load(f)\n",
        "decoder = Decoder(config)\n",
        "train(decoder, config, writer, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "from PIL import Image\n",
        "\n",
        "inv_vocab = dict(zip(range(18), vocabulary))\n",
        "\n",
        "print(inv_vocab)\n",
        "\n",
        "train_set = CustomImageDataset(img_dir='/content/drive/MyDrive/Training/img_dir', text_dir='/content/drive/MyDrive/Training/gui_dir', transform=torchvision.transforms.Resize((224, 224)))\n",
        "train_loader = DataLoader(train_set, batch_size=config[\"batch_size\"][\"train\"], shuffle=True, num_workers=1)\n",
        "\n",
        "encoder = models.resnet50(pretrained=True)\n",
        "# Extract only the convolutional backbone of the model\n",
        "encoder = torch.nn.Sequential(*(list(encoder.children())[:-2]))\n",
        "encoder = encoder.to(device)\n",
        "# Freeze encoder layers\n",
        "for param in encoder.parameters():\n",
        "  param.requires_grad = False\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "img_features = encoder(train_set[1][0].to(device).unsqueeze(0))\n",
        "img_features = img_features.view(img_features.size(0), img_features.size(1), -1)\n",
        "img_features = img_features.permute(0, 2, 1)\n",
        "img_features = img_features.detach()\n",
        "\n",
        "y_pred = greedy_decoding(decoder, img_features, 13, 16, 0, inv_vocab, config['max_len'], device)\n",
        "\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "5EnJyxYu0eeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set[1])"
      ],
      "metadata": {
        "id": "GAkT4UC7ygRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipreqs"
      ],
      "metadata": {
        "id": "Ze_Gq8UHmu_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pipreqs --force ."
      ],
      "metadata": {
        "id": "bQG2O59Km8D_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}