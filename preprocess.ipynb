{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92ee284-e010-47fa-9ac5-0a90faf8d3c5",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a895b7b1-243b-4c85-9c1e-cae9b9aa7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('captions.txt', 'r') as file:\n",
    "    lines = file.readlines()[1:]\n",
    "    \n",
    "images = []\n",
    "captions = []\n",
    "\n",
    "for line in lines:\n",
    "    image, caption = line.strip().split(',', 1)\n",
    "    images.append(image)\n",
    "    captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cfc3d7-09a3-47fb-b53b-c46b5fd2be52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000268201_693b08cb0e.jpg',\n",
       " '1000268201_693b08cb0e.jpg',\n",
       " '1000268201_693b08cb0e.jpg',\n",
       " '1000268201_693b08cb0e.jpg',\n",
       " '1000268201_693b08cb0e.jpg',\n",
       " '1001773457_577c3a7d70.jpg',\n",
       " '1001773457_577c3a7d70.jpg',\n",
       " '1001773457_577c3a7d70.jpg',\n",
       " '1001773457_577c3a7d70.jpg',\n",
       " '1001773457_577c3a7d70.jpg',\n",
       " '1002674143_1b742ab4b8.jpg',\n",
       " '1002674143_1b742ab4b8.jpg',\n",
       " '1002674143_1b742ab4b8.jpg',\n",
       " '1002674143_1b742ab4b8.jpg',\n",
       " '1002674143_1b742ab4b8.jpg',\n",
       " '1003163366_44323f5815.jpg',\n",
       " '1003163366_44323f5815.jpg',\n",
       " '1003163366_44323f5815.jpg',\n",
       " '1003163366_44323f5815.jpg',\n",
       " '1003163366_44323f5815.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2542dbf9-8a63-4175-82e8-73916d85e3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .',\n",
       " 'A black dog and a spotted dog are fighting',\n",
       " 'A black dog and a tri-colored dog playing with each other on the road .',\n",
       " 'A black dog and a white dog with brown spots are staring at each other in the street .',\n",
       " 'Two dogs of different breeds looking at each other on the road .',\n",
       " 'Two dogs on pavement moving toward each other .',\n",
       " 'A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .',\n",
       " 'A little girl is sitting in front of a large painted rainbow .',\n",
       " 'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .',\n",
       " 'There is a girl with pigtails sitting in front of a rainbow painting .',\n",
       " 'Young girl with pigtails painting outside in the grass .',\n",
       " 'A man lays on a bench while his dog sits by him .',\n",
       " 'A man lays on the bench to which a white dog is also tied .',\n",
       " 'a man sleeping on a bench outside with a white and black dog sitting next to him .',\n",
       " 'A shirtless man lies on a park bench with his dog .',\n",
       " 'man laying on bench holding leash of dog sitting on ground .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941ccca-7be6-420a-a7e3-c7c844514739",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd0758-11c6-4858-abe6-b86f73b521db",
   "metadata": {},
   "source": [
    "Documentation for image transformations see: https://pytorch.org/vision/0.15/transforms.html\n",
    "\n",
    "### Standard procedure: \n",
    "\n",
    "1. Convert image to a (3 x Height x Width) tensor.\n",
    "2. Normalize values based on mean and std of each RGB channel.\n",
    "3. Randomly rotate, flip, jitter, and crop the image.\n",
    "4. Repeat step 3 to gain a list of augmented image tensors.\n",
    "5. Convert each tensor to shape (1 x 3 x Height x Width) using `unsqueeze(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47522caa-c922-469d-b6a0-2476bc80a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Apply resizing, normalization, and random series of transformations to generate num augmented images\n",
    "def preprocess(image, size, num):\n",
    "    augmented_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomRotation(degrees=15),  # Random rotation up to 15 degrees\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with a probability of 0.5\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "        transforms.RandomResizedCrop(299, scale=(0.8, 1.0), ratio=(1, 1))  # Random resized crop\n",
    "    ])\n",
    "    processed_images = []\n",
    "    for _ in range(num):\n",
    "        augmented_image = augmented_transforms(image)\n",
    "        batch_input = augmented_image.unsqueeze(0)\n",
    "        processed_images.append(batch_input)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70cb8f-a119-48e9-a9a8-ee0e7ecc66ef",
   "metadata": {},
   "source": [
    "### Feature extraction using pretrained model Inception_V3\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 299. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614a6c6f-6421-440e-9e61-37449c6561f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\micha/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "def encode_image(image):\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "        probabilities = torch.nn.functional.softmax(output)\n",
    "    return output, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45c35994-6a39-4efc-8f24-de3a5392bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368929b-e50f-4144-a5f7-8e4d3dcf0322",
   "metadata": {},
   "source": [
    "Tokenization: convert text sentences to list of dictionary index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb1cf6c1-1038-422b-9f8a-c79d49b2ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text_data):\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    # Tokenize the text data\n",
    "    tokenized_data = [tokenizer(text) for text in text_data]\n",
    "    # Build vocabulary from tokenized data\n",
    "    vocab = build_vocab_from_iterator(tokenized_data, specials=[\"<unk>\", \"<pad>\"])\n",
    "    return tokenized_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92b8479d-05af-4e00-9990-e64f68c18b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_data, vocab = build_vocab(captions)\n",
    "numericalized_data = [torch.tensor([vocab[token] for token in tokens]) for tokens in tokenized_data]\n",
    "\n",
    "print(\"Tokenized Data:\")\n",
    "print(tokenized_data)\n",
    "print(\"Numericalized Data:\")\n",
    "print(numericalized_data)\n",
    "print(\"Vocabulary Size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1dcd15-5a78-4faf-9132-278a310e75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ccf2c5b-bb99-45da-b865-cea59299838c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'stoi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m vector_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load pre-trained GloVe vectors\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# glove = GloVe(name='6B', dim=vector_dim)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoi\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     10\u001b[0m     word_vector \u001b[38;5;241m=\u001b[39m glove[word]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord vector for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_vector\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'stoi'"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Define the vector dimension (50, 100, 200, or 300 are common choices)\n",
    "vector_dim = 100\n",
    "\n",
    "# Load pre-trained GloVe vectors\n",
    "# glove = GloVe(name='6B', dim=vector_dim)\n",
    "\n",
    "for word in vocab.stoi.items():\n",
    "    word_vector = glove[word]\n",
    "    print(f'Word vector for \"{word}\": {word_vector}')\n",
    "    \n",
    "    print(f'Vocabulary size: {len(glove.itos)}')\n",
    "    print(f'Vector dimension: {glove.vectors.size(1)}')\n",
    "\n",
    "# Glove vector model too large to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23a779-3a52-42b1-a792-b101798987c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
