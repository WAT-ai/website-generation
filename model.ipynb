{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodings(nn.Module):\n",
    "    \"\"\"Attention is All You Need positional encoding layer\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, d_model, p_dropout):\n",
    "        \"\"\"Initializes the layer.\"\"\"\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        token_positions = torch.arange(start=0, end=seq_len).view(-1, 1)\n",
    "        dim_positions = torch.arange(start=0, end=d_model).view(1, -1)\n",
    "        angles = token_positions / (10000 ** ((2 * dim_positions) / d_model))\n",
    "\n",
    "        encodings = torch.zeros(1, seq_len, d_model)\n",
    "        encodings[0, :, ::2] = torch.cos(angles[:, ::2])\n",
    "        encodings[0, :, 1::2] = torch.sin(angles[:, 1::2])\n",
    "        encodings.requires_grad = False\n",
    "        self.register_buffer(\"positional_encodings\", encodings)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs forward pass of the module.\"\"\"\n",
    "        x = x + self.positional_encodings\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder for image captions.\n",
    "\n",
    "    Generates prediction for next caption word given the prviously\n",
    "    generated word and image features extracted from CNN.    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        model_config = config[\"model_configuration\"]\n",
    "        decoder_layers = model_config[\"decoder_layers\"]\n",
    "        attention_heads = model_config[\"attention_heads\"]\n",
    "        d_model = model_config[\"d_model\"]\n",
    "        ff_dim = model_config[\"ff_dim\"]\n",
    "        dropout = model_config[\"dropout\"]\n",
    "\n",
    "        embedding_dim = config[\"embeddings\"][\"size\"]\n",
    "        vocab_size = config[\"vocab_size\"]\n",
    "        img_feature_channels = config[\"image_specs\"][\"img_feature_channels\"]\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.entry_mapping_tokens = nn.Linear(embedding_dim, d_model)\n",
    "        self.entry_mapping_img = nn.Linear(img_feature_channels, d_model)\n",
    "\n",
    "        self.res_block = ResidualBlock(d_model)\n",
    "\n",
    "        self.positional_encodings = PositionalEncodings(config[\"max_len\"], d_model, dropout)\n",
    "        transformer_decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=attention_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(transformer_decoder_layer, decoder_layers)\n",
    "        self.classifier = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, image_features, tgt_padding_mask=None, tgt_mask=None):\n",
    "        \"\"\"Performs forward pass of the module.\"\"\"\n",
    "        # Adapt the dimensionality of the features for image patches\n",
    "        image_features = self.entry_mapping_img(image_features)\n",
    "        image_features = image_features.permute(1, 0, 2)\n",
    "        image_features = F.leaky_relu(image_features)\n",
    "\n",
    "        # Entry mapping for word tokens\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.entry_mapping_tokens(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.res_block(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.positional_encodings(x)\n",
    "\n",
    "        # Get output from the decoder\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.decoder(\n",
    "            tgt=x,\n",
    "            memory=image_features,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_causal_mask(seq_len, device):\n",
    "    \"\"\"Defines the triangular mask used in transformers.\n",
    "\n",
    "    This mask prevents decoder from attending the tokens after the current one.\n",
    "\n",
    "    Arguments:\n",
    "        seq_len (int): Maximum length of input sequence\n",
    "        device: Device on which to map the created tensor mask\n",
    "    Returns:\n",
    "        mask (torch.Tensor): Created triangular mask\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "    mask.requires_grad = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(config, writer, device):\n",
    "    \"\"\"Performs the training of the model.\n",
    "\n",
    "    Arguments:\n",
    "        config (object): Contains configuration of the pipeline\n",
    "        writer: tensorboardX writer object\n",
    "        device: device on which to map the model and data\n",
    "    \"\"\"\n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "\n",
    "   # Define dataloader hyper-parameters\n",
    "    train_hyperparams = {\n",
    "        \"batch_size\": config[\"batch_size\"][\"train\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"drop_last\": True\n",
    "    }\n",
    "\n",
    "    # Create dataloaders\n",
    "    # train_set = Flickr8KDataset(config, config[\"split_save\"][\"train\"], training=True)\n",
    "    # valid_set = Flickr8KDataset(config, config[\"split_save\"][\"validation\"], training=False)\n",
    "    train_loader = DataLoader(train_set, **train_hyperparams)\n",
    "\n",
    "    #######################\n",
    "    # Set up the encoder \n",
    "    #######################\n",
    "    # Download pretrained CNN encoder\n",
    "    encoder = models.resnet50(pretrained=True)\n",
    "    # Extract only the convolutional backbone of the model\n",
    "    encoder = torch.nn.Sequential(*(list(encoder.children())[:-2]))\n",
    "    encoder = encoder.to(device)\n",
    "    # Freeze encoder layers\n",
    "    # for param in encoder.parameters():\n",
    "    # param.requires_grad = False\n",
    "    encoder.eval()\n",
    "\n",
    "    ######################\n",
    "    # Set up the decoder\n",
    "    ######################\n",
    "    # Instantiate the decoder\n",
    "    decoder = Decoder(config)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    # Set up causal mask for transformer decoder\n",
    "    causal_mask = set_up_causal_mask(config[\"max_len\"], device)\n",
    "\n",
    "    # Load training configuration\n",
    "    train_config = config[\"train_config\"]\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "\n",
    "    # Prepare the model optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        decoder.parameters(),\n",
    "        lr=train_config[\"learning_rate\"],\n",
    "        weight_decay=train_config[\"l2_penalty\"]\n",
    "    )\n",
    "    # Loss function\n",
    "    loss_fcn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    start_time = time.strftime(\"%b-%d_%H-%M-%S\")\n",
    "    train_step = 0\n",
    "    for epoch in range(train_config[\"num_of_epochs\"]):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        decoder.train()\n",
    "\n",
    "        for x_img, x_words, y, tgt_padding_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            train_step += 1\n",
    "\n",
    "            # Move the used tensors to defined device\n",
    "            x_img, x_words = x_img.to(device), x_words.to(device)\n",
    "            y = y.to(device)\n",
    "            tgt_padding_mask = tgt_padding_mask.to(device)\n",
    "\n",
    "            # Extract image features\n",
    "            '''\n",
    "            with torch.no_grad():\n",
    "                img_features = encoder(x_img)\n",
    "                img_features = img_features.view(img_features.size(0), img_features.size(1), -1)\n",
    "                img_features = img_features.permute(0, 2, 1)\n",
    "                img_features = img_features.detach()\n",
    "            '''\n",
    "\n",
    "            img_features = encoder(x_img)\n",
    "            img_features = img_features.view(img_features.size(0), img_features.size(1), -1)\n",
    "            img_features = img_features.permute(0, 2, 1)\n",
    "            img_features = img_features.detach()\n",
    "            # Get the prediction of the decoder\n",
    "            y_pred = decoder(x_words, img_features)\n",
    "            # tgt_padding_mask = torch.logical_not(tgt_padding_mask)\n",
    "            # y_pred = y_pred[tgt_padding_mask]\n",
    "\n",
    "            # y = y[tgt_padding_mask]\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fcn(y_pred, y.long())\n",
    "\n",
    "            # Update model weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"Train/Step-Loss\", loss.item(), train_step)\n",
    "            writer.add_scalar(\"Train/Learning-Rate\", learning_rate, train_step)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
